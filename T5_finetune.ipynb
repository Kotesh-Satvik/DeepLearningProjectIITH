{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a229aa8-ac9b-4fd9-be9b-ee5dbab9d13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA supported by this system? True\n",
      "CUDA version: 12.1\n",
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda:1\")\n",
    "torch.cuda.set_device(DEVICE)\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2039d42a-9e5c-449d-838e-59eba1f35247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (4.41.0.dev0)\n",
      "Requirement already satisfied: filelock in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (0.4.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from evaluate) (2.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from evaluate) (2.2.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from evaluate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from evaluate) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from evaluate) (0.22.0)\n",
      "Requirement already satisfied: packaging in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from evaluate) (23.2)\n",
      "Requirement already satisfied: responses<0.19 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: filelock in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from pandas->evaluate) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: six>=1.5 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: six in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from rouge) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from spacy) (2.7.0)\n",
      "Requirement already satisfied: jinja2 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from spacy) (69.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /DATA1/ai20btech11028/miniconda3/envs/jupyter/lib/python3.11/site-packages (from jinja2->spacy) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install evaluate\n",
    "!pip install rouge\n",
    "!pip install spacy\n",
    "\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "import evaluate  # Bleu\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import T5Tokenizer, T5Model, T5ForConditionalGeneration, T5TokenizerFast\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a8da2f3-67cc-456f-84c4-5cad9c9d66e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_name):\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    \n",
    "    df = []\n",
    "    df_multi = []\n",
    "    df_phrase = []\n",
    "    df_passage = []\n",
    "    with open('/DATA1/ai20btech11028/vojes_nlp/Data/' + file_name, encoding='utf-8') as f:\n",
    "        for i in f:\n",
    "            i = json.loads(i)\n",
    "                \n",
    "            tweet = i['postText']\n",
    "            article_title = i['targetTitle']\n",
    "            article = ' '.join(i['targetParagraphs'])\n",
    "            spoilers = i['spoiler']\n",
    "            label = i['tags']\n",
    "            keywords = i['targetKeywords']\n",
    "            positions = i['spoilerPositions']\n",
    "            \n",
    "            assert len(tweet) == 1\n",
    "            tweet = tweet[0]\n",
    "            \n",
    "            assert len(label) == 1\n",
    "            label = label[0]\n",
    "            \n",
    "            if label not in ['phrase', 'phrases', 'passage', 'multi']:\n",
    "                print(label)\n",
    "                \n",
    "            assert label in ['phrase', 'phrases', 'passage', 'multi']\n",
    "            \n",
    "            if label == 'phrase' or label == 'phrases':\n",
    "                label = 0\n",
    "            elif label == 'multi':\n",
    "                label = 2\n",
    "            else:\n",
    "                label = 1\n",
    "                \n",
    "            \n",
    "            df += [{'question': tweet , 'context': i['targetParagraphs'], 'article': article_title + article, 'spoiler':spoilers,\n",
    "                    'labels': label, 'keywords':keywords, 'spoilerPositions':positions}]\n",
    "\n",
    "    return pd.DataFrame(df)\n",
    "            \n",
    "    \n",
    "# # test_dataset = load_dataset('test.jsonl')\n",
    "train_dataset = load_dataset('train.jsonl')\n",
    "validation_dataset = load_dataset('validation.jsonl')\n",
    "# print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90328edd-15e2-49db-a3a1-57f4b52b2d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"MateuszW/generated_questions\")\n",
    "\n",
    "genQue_train = dataset['train']['generated_questions']\n",
    "genQue_test = dataset['validation']['generated_questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38550415-ab26-4ed7-b7f4-fd6d110d70ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>article</th>\n",
       "      <th>spoiler</th>\n",
       "      <th>labels</th>\n",
       "      <th>keywords</th>\n",
       "      <th>spoilerPositions</th>\n",
       "      <th>generated_que</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wes Welker Wanted Dinner With Tom Brady, But P...</td>\n",
       "      <td>[It’ll be just like old times this weekend for...</td>\n",
       "      <td>Wes Welker Wanted Dinner With Tom Brady, But P...</td>\n",
       "      <td>[how about that morning we go throw?]</td>\n",
       "      <td>1</td>\n",
       "      <td>new england patriots, ricky doyle, top stories,</td>\n",
       "      <td>[[[3, 151], [3, 186]]]</td>\n",
       "      <td>What did Tom Brady do instead of having dinner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NASA sets date for full recovery of ozone hole</td>\n",
       "      <td>[2070 is shaping up to be a great year for Mot...</td>\n",
       "      <td>Hole In Ozone Layer Expected To Make Full Reco...</td>\n",
       "      <td>[2070]</td>\n",
       "      <td>0</td>\n",
       "      <td>ozone layer,ozone hole determined by weather,M...</td>\n",
       "      <td>[[[0, 0], [0, 4]]]</td>\n",
       "      <td>What is the date that NASA has set for the ful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is what makes employees happy -- and it's...</td>\n",
       "      <td>[Despite common belief, money isn't the key to...</td>\n",
       "      <td>Intellectual Stimulation Trumps Money For Empl...</td>\n",
       "      <td>[intellectual stimulation]</td>\n",
       "      <td>0</td>\n",
       "      <td>employee happiness money,employee happiness in...</td>\n",
       "      <td>[[[1, 186], [1, 210]]]</td>\n",
       "      <td>What makes employees happy?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Passion is overrated — 7 work habits you need ...</td>\n",
       "      <td>[It’s common wisdom. Near gospel really, and n...</td>\n",
       "      <td>‘Follow your passion’ is wrong, here are 7 hab...</td>\n",
       "      <td>[Purpose connects us to something bigger and i...</td>\n",
       "      <td>2</td>\n",
       "      <td>business, work-life, careers</td>\n",
       "      <td>[[[11, 25], [11, 101]], [[17, 56], [17, 85]], ...</td>\n",
       "      <td>What are the 7 work habits that are considered...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The perfect way to cook rice so that it's perf...</td>\n",
       "      <td>[Boiling rice may seem simple, but there is a ...</td>\n",
       "      <td>Revealed: The perfect way to cook rice so that...</td>\n",
       "      <td>[in a rice cooker]</td>\n",
       "      <td>0</td>\n",
       "      <td>Quora,users,share,perfect,way,cook,rice</td>\n",
       "      <td>[[[5, 60], [5, 76]]]</td>\n",
       "      <td>What is the best way to cook rice so that it's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>Has Facebook's video explosion completely shak...</td>\n",
       "      <td>[A long time ago in a galaxy far, far away...W...</td>\n",
       "      <td>Facebook Video Surging, But YouTube Still Offe...</td>\n",
       "      <td>[it hasn’t necessarily taken the wind out of Y...</td>\n",
       "      <td>1</td>\n",
       "      <td>Facebook,web video,web video ads,YouTube</td>\n",
       "      <td>[[[7, 50], [7, 118]]]</td>\n",
       "      <td>Has Facebook's video explosion completely shak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>Cop Is Eating At A Chili's When Teen Hands Him...</td>\n",
       "      <td>[The Kansas City, Kansas Police Department are...</td>\n",
       "      <td>Cop is eating at Chili's when teen hands him f...</td>\n",
       "      <td>[It read, \"Thanks for keeping us safe.\"]</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>[[[0, 317], [0, 355]]]</td>\n",
       "      <td>What did the teen give the cop?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>5 popular myths about visible signs of aging t...</td>\n",
       "      <td>[Obama looks decades younger already, but what...</td>\n",
       "      <td>5 popular myths about visible signs of aging t...</td>\n",
       "      <td>[1. Anti-wrinkle creams will erase the fine li...</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>[[[6, 0], [6, 73]], [[10, 0], [10, 109]], [[14...</td>\n",
       "      <td>What are the 5 popular myths about visible sig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3198</th>\n",
       "      <td>You need to see this Twitter account that pred...</td>\n",
       "      <td>[What the HELL?!, 1. Unless you’re somewhere w...</td>\n",
       "      <td>WTF, It Looks Like This Twitter Account \"Predi...</td>\n",
       "      <td>[@beyoncefan666]</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>[[[3, 55], [3, 69]]]</td>\n",
       "      <td>What is the purpose of the Twitter account men...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>GOP congressman comes out for gay marriage</td>\n",
       "      <td>[Rep. Charlie Dent (R-Pa.) came out in support...</td>\n",
       "      <td>Pennsylvania GOP Rep. Charlie Dent Comes Out F...</td>\n",
       "      <td>[Rep. Charlie Dent (R-Pa.)]</td>\n",
       "      <td>0</td>\n",
       "      <td>lgbt,charlie dent gay marriage,Charlie Dent,pe...</td>\n",
       "      <td>[[[0, 0], [0, 25]]]</td>\n",
       "      <td>What is the name of the GOP congressman who ca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3200 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0     Wes Welker Wanted Dinner With Tom Brady, But P...   \n",
       "1        NASA sets date for full recovery of ozone hole   \n",
       "2     This is what makes employees happy -- and it's...   \n",
       "3     Passion is overrated — 7 work habits you need ...   \n",
       "4     The perfect way to cook rice so that it's perf...   \n",
       "...                                                 ...   \n",
       "3195  Has Facebook's video explosion completely shak...   \n",
       "3196  Cop Is Eating At A Chili's When Teen Hands Him...   \n",
       "3197  5 popular myths about visible signs of aging t...   \n",
       "3198  You need to see this Twitter account that pred...   \n",
       "3199         GOP congressman comes out for gay marriage   \n",
       "\n",
       "                                                context  \\\n",
       "0     [It’ll be just like old times this weekend for...   \n",
       "1     [2070 is shaping up to be a great year for Mot...   \n",
       "2     [Despite common belief, money isn't the key to...   \n",
       "3     [It’s common wisdom. Near gospel really, and n...   \n",
       "4     [Boiling rice may seem simple, but there is a ...   \n",
       "...                                                 ...   \n",
       "3195  [A long time ago in a galaxy far, far away...W...   \n",
       "3196  [The Kansas City, Kansas Police Department are...   \n",
       "3197  [Obama looks decades younger already, but what...   \n",
       "3198  [What the HELL?!, 1. Unless you’re somewhere w...   \n",
       "3199  [Rep. Charlie Dent (R-Pa.) came out in support...   \n",
       "\n",
       "                                                article  \\\n",
       "0     Wes Welker Wanted Dinner With Tom Brady, But P...   \n",
       "1     Hole In Ozone Layer Expected To Make Full Reco...   \n",
       "2     Intellectual Stimulation Trumps Money For Empl...   \n",
       "3     ‘Follow your passion’ is wrong, here are 7 hab...   \n",
       "4     Revealed: The perfect way to cook rice so that...   \n",
       "...                                                 ...   \n",
       "3195  Facebook Video Surging, But YouTube Still Offe...   \n",
       "3196  Cop is eating at Chili's when teen hands him f...   \n",
       "3197  5 popular myths about visible signs of aging t...   \n",
       "3198  WTF, It Looks Like This Twitter Account \"Predi...   \n",
       "3199  Pennsylvania GOP Rep. Charlie Dent Comes Out F...   \n",
       "\n",
       "                                                spoiler  labels  \\\n",
       "0                 [how about that morning we go throw?]       1   \n",
       "1                                                [2070]       0   \n",
       "2                            [intellectual stimulation]       0   \n",
       "3     [Purpose connects us to something bigger and i...       2   \n",
       "4                                    [in a rice cooker]       0   \n",
       "...                                                 ...     ...   \n",
       "3195  [it hasn’t necessarily taken the wind out of Y...       1   \n",
       "3196           [It read, \"Thanks for keeping us safe.\"]       1   \n",
       "3197  [1. Anti-wrinkle creams will erase the fine li...       2   \n",
       "3198                                   [@beyoncefan666]       0   \n",
       "3199                        [Rep. Charlie Dent (R-Pa.)]       0   \n",
       "\n",
       "                                               keywords  \\\n",
       "0       new england patriots, ricky doyle, top stories,   \n",
       "1     ozone layer,ozone hole determined by weather,M...   \n",
       "2     employee happiness money,employee happiness in...   \n",
       "3                          business, work-life, careers   \n",
       "4               Quora,users,share,perfect,way,cook,rice   \n",
       "...                                                 ...   \n",
       "3195           Facebook,web video,web video ads,YouTube   \n",
       "3196                                               None   \n",
       "3197                                                      \n",
       "3198                                                      \n",
       "3199  lgbt,charlie dent gay marriage,Charlie Dent,pe...   \n",
       "\n",
       "                                       spoilerPositions  \\\n",
       "0                                [[[3, 151], [3, 186]]]   \n",
       "1                                    [[[0, 0], [0, 4]]]   \n",
       "2                                [[[1, 186], [1, 210]]]   \n",
       "3     [[[11, 25], [11, 101]], [[17, 56], [17, 85]], ...   \n",
       "4                                  [[[5, 60], [5, 76]]]   \n",
       "...                                                 ...   \n",
       "3195                              [[[7, 50], [7, 118]]]   \n",
       "3196                             [[[0, 317], [0, 355]]]   \n",
       "3197  [[[6, 0], [6, 73]], [[10, 0], [10, 109]], [[14...   \n",
       "3198                               [[[3, 55], [3, 69]]]   \n",
       "3199                                [[[0, 0], [0, 25]]]   \n",
       "\n",
       "                                          generated_que  \n",
       "0     What did Tom Brady do instead of having dinner...  \n",
       "1     What is the date that NASA has set for the ful...  \n",
       "2                           What makes employees happy?  \n",
       "3     What are the 7 work habits that are considered...  \n",
       "4     What is the best way to cook rice so that it's...  \n",
       "...                                                 ...  \n",
       "3195  Has Facebook's video explosion completely shak...  \n",
       "3196                    What did the teen give the cop?  \n",
       "3197  What are the 5 popular myths about visible sig...  \n",
       "3198  What is the purpose of the Twitter account men...  \n",
       "3199  What is the name of the GOP congressman who ca...  \n",
       "\n",
       "[3200 rows x 8 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"generated_que\"] = list(genQue_train)\n",
    "validation_dataset[\"generated_que\"] = list(genQue_test)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0cacdae-39c3-4053-a097-3a0cf536c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
    "MODEL = T5ForConditionalGeneration.from_pretrained(\"t5-base\", return_dict=True).to(DEVICE)\n",
    "OPTIMIZER = Adam(MODEL.parameters(), lr=0.0001)\n",
    "Q_LEN = 256   # Question Length\n",
    "T_LEN = 128    # Target Length\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc668276-b378-4ceb-8f51-c54e76b7eb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting context, question, and answers from the dataset\n",
    "\n",
    "def prepare_data(train_dataset):\n",
    "    articles = []\n",
    "    \n",
    "    for i in range(len(train_dataset)):\n",
    "        \n",
    "        clickbait = train_dataset.iloc[i]\n",
    "        context = clickbait['article']\n",
    "        question = clickbait['generated_que']\n",
    "        answer = ' '.join(clickbait['spoiler'])\n",
    "\n",
    "        inputs = {\"context\": context, \"question\": question, \"answer\": answer}\n",
    "\n",
    "        articles.append(inputs)\n",
    "\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38aa81d4-7848-417d-887e-441acce49c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_data(train_dataset)\n",
    "\n",
    "# Create a Dataframe\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7180858a-c506-45dd-837c-fda51e840e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QA_Dataset(Dataset):\n",
    "    def __init__(self, tokenizer, dataframe, q_len, t_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.q_len = q_len\n",
    "        self.t_len = t_len\n",
    "        self.data = dataframe\n",
    "        self.questions = self.data[\"question\"]\n",
    "        self.context = self.data[\"context\"]\n",
    "        self.answer = self.data['answer']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        context = self.context[idx]\n",
    "        answer = self.answer[idx]\n",
    "        \n",
    "        question_tokenized = self.tokenizer(question, context, max_length=self.q_len, padding=\"max_length\",\n",
    "                                                    truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
    "        answer_tokenized = self.tokenizer(answer, max_length=self.t_len, padding=\"max_length\", \n",
    "                                          truncation=True, pad_to_max_length=True, add_special_tokens=True)\n",
    "        \n",
    "        labels = torch.tensor(answer_tokenized[\"input_ids\"], dtype=torch.long)\n",
    "        labels[labels == 0] = -100\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(question_tokenized[\"input_ids\"], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(question_tokenized[\"attention_mask\"], dtype=torch.long),\n",
    "            \"labels\": labels,\n",
    "            \"decoder_attention_mask\": torch.tensor(answer_tokenized[\"attention_mask\"], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "519d6b1d-9e9e-4fd1-a29c-b496bd562140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_sampler = RandomSampler(train_data.index)\n",
    "val_sampler = RandomSampler(val_data.index)\n",
    "\n",
    "qa_dataset = QA_Dataset(TOKENIZER, data, Q_LEN, T_LEN)\n",
    "\n",
    "train_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "val_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59d50f79-7ad0-47a0-a5ca-319359b09158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|███████████████████████████████████████████████████████████████| 160/160 [01:01<00:00,  2.59it/s]\n",
      "Validation batches: 100%|███████████████████████████████████████████████████████████████| 40/40 [00:14<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10 -> Train loss: 1.9546649016439914\tValidation loss: 1.4414264179766179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|███████████████████████████████████████████████████████████████| 160/160 [01:01<00:00,  2.59it/s]\n",
      "Validation batches: 100%|███████████████████████████████████████████████████████████████| 40/40 [00:14<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/10 -> Train loss: 1.7599840533919633\tValidation loss: 1.2159628041088582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|███████████████████████████████████████████████████████████████| 160/160 [01:01<00:00,  2.59it/s]\n",
      "Validation batches: 100%|███████████████████████████████████████████████████████████████| 40/40 [00:15<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/10 -> Train loss: 1.627574315480888\tValidation loss: 1.0374203854550919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|███████████████████████████████████████████████████████████████| 160/160 [01:02<00:00,  2.56it/s]\n",
      "Validation batches: 100%|███████████████████████████████████████████████████████████████| 40/40 [00:14<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/10 -> Train loss: 1.5205747211817653\tValidation loss: 0.8858056117314845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training batches: 100%|███████████████████████████████████████████████████████████████| 160/160 [01:02<00:00,  2.55it/s]\n",
      "Validation batches: 100%|███████████████████████████████████████████████████████████████| 40/40 [00:15<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/10 -> Train loss: 1.424874269273132\tValidation loss: 0.7601377387903631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_loss = 0\n",
    "val_loss = 0\n",
    "train_batch_count = 0\n",
    "val_batch_count = 0\n",
    "\n",
    "for epoch in range(5):\n",
    "    MODEL.train()\n",
    "    for batch in tqdm(train_loader, desc=\"Training batches\"):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n",
    "\n",
    "        outputs = MODEL(\n",
    "                          input_ids=input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          labels=labels,\n",
    "                          decoder_attention_mask=decoder_attention_mask\n",
    "                        )\n",
    "\n",
    "        OPTIMIZER.zero_grad()\n",
    "        outputs.loss.backward()\n",
    "        OPTIMIZER.step()\n",
    "        train_loss += outputs.loss.item()\n",
    "        train_batch_count += 1\n",
    "    \n",
    "    #Evaluation\n",
    "    MODEL.eval()\n",
    "    for batch in tqdm(val_loader, desc=\"Validation batches\"):\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n",
    "\n",
    "        outputs = MODEL(\n",
    "                          input_ids=input_ids,\n",
    "                          attention_mask=attention_mask,\n",
    "                          labels=labels,\n",
    "                          decoder_attention_mask=decoder_attention_mask\n",
    "                        )\n",
    "\n",
    "        OPTIMIZER.zero_grad()\n",
    "        outputs.loss.backward()\n",
    "        OPTIMIZER.step()\n",
    "        val_loss += outputs.loss.item()\n",
    "        val_batch_count += 1\n",
    "        \n",
    "    print(f\"{epoch+1}/{10} -> Train loss: {train_loss / train_batch_count}\\tValidation loss: {val_loss/val_batch_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0c3a4bd-f926-41d5-8e08-e84b86aadb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer(context, question, ref_answer=None):\n",
    "    inputs = TOKENIZER(question, context, max_length=Q_LEN, padding=\"max_length\", truncation=True, add_special_tokens=True)\n",
    "    \n",
    "    input_ids = torch.tensor(inputs[\"input_ids\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n",
    "    attention_mask = torch.tensor(inputs[\"attention_mask\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n",
    "\n",
    "    outputs = MODEL.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "  \n",
    "    predicted_answer = TOKENIZER.decode(outputs.flatten(), skip_special_tokens=True)\n",
    "    \n",
    "    if ref_answer:\n",
    "        # Load the Bleu metric\n",
    "        bleu = evaluate.load(\"google_bleu\")\n",
    "        rouge = evaluate.load('rouge')\n",
    "        bert = evaluate.load(\"bertscore\")\n",
    "        score = bleu.compute(predictions=[predicted_answer], \n",
    "                            references=[ref_answer])\n",
    "        bert_score = bert.compute(predictions=[predicted_answer], references=[ref_answer], lang='en', device='cuda:1')\n",
    "        \n",
    "        # score_rouge = rouge.compute(predictioins=[predicted_answer],\n",
    "                                   # references = [ref_answer])\n",
    "    \n",
    "        # print(\"Context: \\n\", context)\n",
    "        # |print(\"\\n\")\n",
    "        print(\"Question: \\n\", question)\n",
    "        return { \"Reference Answer: \": ref_answer, \n",
    "            \"Predicted Answer: \": predicted_answer, \n",
    "            \"BLEU Score: \": score,\n",
    "            \"Bert Score: \": bert_score\n",
    "        }\n",
    "    else:\n",
    "        return predicted_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e766513f-9cd3-4a73-a04c-8b536a0333aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- 0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the reason for the delay of the sequel to Five Nights at Freddy's?\n",
      "{'Reference Answer: ': 'some of the plot elements are so disturbing that they are making him feel sick', 'Predicted Answer: ': 'too dark', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.8500030040740967], 'recall': [0.8265589475631714], 'f1': [0.8381170630455017], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the word that could determine the fate of Arizona Sheriff Joe Arpaio?\n",
      "{'Reference Answer: ': '\"intentionally\" could transform a court case against Phoenix-area Sheriff Joe Arpaio from civil charges to a criminal prosecution', 'Predicted Answer: ': '\"intentionally\"', 'BLEU Score: ': {'google_bleu': 0.08108108108108109}, 'Bert Score: ': {'precision': [0.9389325976371765], 'recall': [0.8278279304504395], 'f1': [0.8798867464065552], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 2\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " How much should you be tipping your hairdresser?\n",
      "{'Reference Answer: ': '20%', 'Predicted Answer: ': '20%', 'BLEU Score: ': {'google_bleu': 1.0}, 'Bert Score: ': {'precision': [0.9999998807907104], 'recall': [0.9999998807907104], 'f1': [0.9999998807907104], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 3\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the name of the new movie in which \"Harry Potter\" alums are reuniting?\n",
      "{'Reference Answer: ': 'Alan Rickman & Rupert Grint CBGB', 'Predicted Answer: ': 'Alan Rickman', 'BLEU Score: ': {'google_bleu': 0.16666666666666666}, 'Bert Score: ': {'precision': [0.97617506980896], 'recall': [0.8567665815353394], 'f1': [0.9125813841819763], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 4\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What happened next?\n",
      "{'Reference Answer: ': 'a man who swallowed a 64GB microSD card and then pooped it into a strainer', 'Predicted Answer: ': '64GB microSD card', 'BLEU Score: ': {'google_bleu': 0.1111111111111111}, 'Bert Score: ': {'precision': [0.8923825025558472], 'recall': [0.8183907866477966], 'f1': [0.8537865281105042], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 5\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the name of the popular soda that scientists say could cure hangovers?\n",
      "{'Reference Answer: ': 'Sprite', 'Predicted Answer: ': 'Sprite', 'BLEU Score: ': {'google_bleu': 1.0}, 'Bert Score: ': {'precision': [0.9999999403953552], 'recall': [0.9999999403953552], 'f1': [0.9999999403953552], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 6\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the anytime snack?\n",
      "{'Reference Answer: ': 'Smoky Paprika-Baked Garbanzo Beans', 'Predicted Answer: ': 'Smoky Paprika-Baked Garbanzo Beans', 'BLEU Score: ': {'google_bleu': 1.0}, 'Bert Score: ': {'precision': [1.0], 'recall': [1.0], 'f1': [1.0], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 7\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the stunning \"Harry Potter\" revelation about Professor McGonagall?\n",
      "{'Reference Answer: ': 'McGonagall was appointed as Dumbledore’s assistant in 1956, not as his replacement.', 'Predicted Answer: ': 'originally hired by Dumbledore to teach Defense Against the Dark Arts (DADA', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.83613121509552], 'recall': [0.8422108292579651], 'f1': [0.8391600251197815], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 8\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the answer that J.J. Abrams has given about the post-credits scene in the new 'Star Wars'?\n",
      "{'Reference Answer: ': 'All the scenes are actually in the movie', 'Predicted Answer: ': 'No, there’s not. All the scenes are actually in the movie.', 'BLEU Score: ': {'google_bleu': 0.52}, 'Bert Score: ': {'precision': [0.8897451162338257], 'recall': [0.9339311122894287], 'f1': [0.9113028049468994], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 9\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What did Kristin Cavallari say about \"The Hills\"?\n",
      "{'Reference Answer: ': '\"I had fake relationships, fake fights. I don\\'t care anymore, I can tell you.', 'Predicted Answer: ': '\"I had fake relationships, fake fights. I don\\'t care anymore, I can', 'BLEU Score: ': {'google_bleu': 0.8285714285714286}, 'Bert Score: ': {'precision': [0.9731026887893677], 'recall': [0.9578542113304138], 'f1': [0.96541827917099], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 10\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " How does this model prepare for lingerie shoots?\n",
      "{'Reference Answer: ': 'Elettra Wiedemann extra strength work, so weights, and quite a few planks for my core. My diet stayed pretty much the same, except I cut out sugar for the week of the shoot', 'Predicted Answer: ': 'Elettra Wiedemann \"It’s kind of like a superhero suit,', 'BLEU Score: ': {'google_bleu': 0.04225352112676056}, 'Bert Score: ': {'precision': [0.9086392521858215], 'recall': [0.8555470108985901], 'f1': [0.8812942504882812], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 11\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " Who did Obama just dine with in Vietnam?\n",
      "{'Reference Answer: ': 'Anthony Bourdain', 'Predicted Answer: ': 'Anthony Bourdain', 'BLEU Score: ': {'google_bleu': 1.0}, 'Bert Score: ': {'precision': [1.0], 'recall': [1.0], 'f1': [1.0], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 12\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the cause of death in the sentence?\n",
      "{'Reference Answer: ': \"he'd eaten a peanut butter sandwich and wasn't aware of her peanut allergy\", 'Predicted Answer: ': 'Myriam Ducre-Lemay, 20, died in 2012 after kissing her boyfriend', 'BLEU Score: ': {'google_bleu': 0.021739130434782608}, 'Bert Score: ': {'precision': [0.7836421728134155], 'recall': [0.8445446491241455], 'f1': [0.8129544258117676], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 13\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the question that the former O.J. Simpson prosecutor hates being asked?\n",
      "{'Reference Answer: ': 'Marcia Clark Does she think Simpson really did it?', 'Predicted Answer: ': 'Marcia Clark', 'BLEU Score: ': {'google_bleu': 0.08823529411764706}, 'Bert Score: ': {'precision': [0.976852536201477], 'recall': [0.8640161156654358], 'f1': [0.9169760942459106], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 14\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the amazing home of the Hollywood legend on the market?\n",
      "{'Reference Answer: ': 'Mitzi Gaynor Beverly Hills, California', 'Predicted Answer: ': 'Beverly Hills, California home', 'BLEU Score: ': {'google_bleu': 0.5555555555555556}, 'Bert Score: ': {'precision': [0.8674613833427429], 'recall': [0.8036143779754639], 'f1': [0.8343181610107422], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 15\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What are mosquito-control workers spraying in Miami?\n",
      "{'Reference Answer: ': 'Dibrom', 'Predicted Answer: ': 'naled', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.8310245871543884], 'recall': [0.7743184566497803], 'f1': [0.8016699552536011], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 16\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the question being asked in the sentence?\n",
      "{'Reference Answer: ': 'They don’t fart', 'Predicted Answer: ': 'They don’t fart—detectably', 'BLEU Score: ': {'google_bleu': 0.5}, 'Bert Score: ': {'precision': [0.8987493515014648], 'recall': [0.9803812503814697], 'f1': [0.937792181968689], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 17\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the most that the angry ex-boyfriend did?\n",
      "{'Reference Answer: ': 'kicked her and got into a fight with her current boyfriend', 'Predicted Answer: ': 'YELL OBNSCRITEVIES', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.7546825408935547], 'recall': [0.8251369595527649], 'f1': [0.7883387804031372], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 18\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the shocking move made by the local cops against the boy selling teddy bear to buy food?\n",
      "{'Reference Answer: ': 'Dunham picked the boy up and took him to a Subway to get something to eat. He then took him to the Franklin Police Department.', 'Predicted Answer: ': 'Officer Steve Dunham', 'BLEU Score: ': {'google_bleu': 0.00980392156862745}, 'Bert Score: ': {'precision': [0.8270367383956909], 'recall': [0.8167792558670044], 'f1': [0.8218759894371033], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 19\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the secret that parents discovered in their adoptive daughter's family tree?\n",
      "{'Reference Answer: ': 'Not only does Aubrey have cerebral palsy, but she was neglected and abused by her biological mother.', 'Predicted Answer: ': 'Aubrey Lumpkins', 'BLEU Score: ': {'google_bleu': 0.014285714285714285}, 'Bert Score: ': {'precision': [0.7948386073112488], 'recall': [0.8118758201599121], 'f1': [0.8032668232917786], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 20\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the one morning work mistake you can't recover from?\n",
      "{'Reference Answer: ': 'starts later', 'Predicted Answer: ': 'rolling into the office around 9:30 a.m. every day may be killing your career', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.8072632551193237], 'recall': [0.8086380958557129], 'f1': [0.8079500794410706], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 21\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is melatonin?\n",
      "{'Reference Answer: ': \"The bottom line: Unfortunately, there's not enough solid research out there on whether melatonin supplements are truly an effective and safe way to get to sleep.\", 'Predicted Answer: ': 'melatonin supplements', 'BLEU Score: ': {'google_bleu': 0.02727272727272727}, 'Bert Score: ': {'precision': [0.9037519097328186], 'recall': [0.8167167901992798], 'f1': [0.8580328226089478], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 22\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What makes the author think that there will probably never be a \"Dawson's Creek\" reunion?\n",
      "{'Reference Answer: ': \"Kevin Williamson said he didn't want to write it\", 'Predicted Answer: ': 'James Van Der Beek', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.7375540733337402], 'recall': [0.8202265501022339], 'f1': [0.7766966223716736], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 23\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the bombshell about the babies that the doctors drop?\n",
      "{'Reference Answer: ': 'Sophie and Riley were considered \"micro-preemies\" and suffered a slew of health issues, like chronic lung disease and holes in their hearts.', 'Predicted Answer: ': 'Sophie and Riley were considered \"micro-preemies.\"', 'BLEU Score: ': {'google_bleu': 0.24489795918367346}, 'Bert Score: ': {'precision': [0.9777225852012634], 'recall': [0.8935332298278809], 'f1': [0.9337339997291565], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 24\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the surprise in Apple iOS 9.3.2?\n",
      "{'Reference Answer: ': 'bricking iPad Pros', 'Predicted Answer: ': 'bricking iPad Pros', 'BLEU Score: ': {'google_bleu': 1.0}, 'Bert Score: ': {'precision': [1.0], 'recall': [1.0], 'f1': [1.0], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 25\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the main topic of the sentence?\n",
      "{'Reference Answer: ': 'Stace Nelson', 'Predicted Answer: ': \"Former South Dakota Gov. Mike Rounds (R) won the Republican Party's nomination in\", 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.7879540920257568], 'recall': [0.7818402051925659], 'f1': [0.7848852276802063], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 26\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " How much do these Instagram models make?\n",
      "{'Reference Answer: ': 'Paige Hathaway – £3.8million Chantel Zales – £3.6 million Ana Cheri – £2.4 million Abigail Ratchford – £2.3 million Claudia Alende – £2.1 million', 'Predicted Answer: ': 'Paige Hathaway £3.8million £3.8million £3.8million £3.8million £3.8', 'BLEU Score: ': {'google_bleu': 0.044444444444444446}, 'Bert Score: ': {'precision': [0.8927661776542664], 'recall': [0.8431088924407959], 'f1': [0.8672271966934204], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 27\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the conflict mentioned in the sentence?\n",
      "{'Reference Answer: ': 'Buss and Jackson announced that they were mutually ending their four-year engagement', 'Predicted Answer: ': \"Phil and Jeanie -- basketball's ultimate power couple -- call it quits\", 'BLEU Score: ': {'google_bleu': 0.023809523809523808}, 'Bert Score: ': {'precision': [0.8498992323875427], 'recall': [0.8566823601722717], 'f1': [0.853277325630188], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 28\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What happened next after the tourist ignored the \"Please Don't Touch\" sign in the museum?\n",
      "{'Reference Answer: ': 'In the video, an abstract, wooden sculpture clock appeared not to be working, so a male tourist decided to take matters into his own hands, pulling on weights and levers for more than 30 seconds before the clock came flying off the wall and into pieces on the floor.', 'Predicted Answer: ': 'In the video, an abstract, wooden sculpture clock appeared not to be working, so a', 'BLEU Score: ': {'google_bleu': 0.3142857142857143}, 'Bert Score: ': {'precision': [0.9679954051971436], 'recall': [0.8841516971588135], 'f1': [0.9241758584976196], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 29\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the newest menu item at Taco Bell and what is the ridiculous ingredient?\n",
      "{'Reference Answer: ': 'reduced fat sour cream', 'Predicted Answer: ': '\"reduced fat sour cream.\"', 'BLEU Score: ': {'google_bleu': 0.45454545454545453}, 'Bert Score: ': {'precision': [0.8867658376693726], 'recall': [0.9355646371841431], 'f1': [0.91051185131073], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 30\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the name of the person mentioned in the sentence?\n",
      "{'Reference Answer: ': 'Brooks would eventually return to a role at News Corp, few expected her to land at Storyful', 'Predicted Answer: ': 'Rebekah Brooks', 'BLEU Score: ': {'google_bleu': 0.015151515151515152}, 'Bert Score: ': {'precision': [0.7202867269515991], 'recall': [0.8078612089157104], 'f1': [0.7615646123886108], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 31\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the name of the children's book illustrator who just got his own exhibition in Chicago?\n",
      "{'Reference Answer: ': 'Edward Gorey', 'Predicted Answer: ': 'Edward Gorey', 'BLEU Score: ': {'google_bleu': 1.0}, 'Bert Score: ': {'precision': [1.0], 'recall': [1.0], 'f1': [1.0], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 32\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the name of the fashion brand that lets models go un-Photoshopped and makeup-free?\n",
      "{'Reference Answer: ': 'Rag & Bone', 'Predicted Answer: ': 'DIY Project', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.8696478605270386], 'recall': [0.7588164210319519], 'f1': [0.81046062707901], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 33\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the Hollywood's biggest trend that Jennifer Lawrence debuted?\n",
      "{'Reference Answer: ': 'pixie cut', 'Predicted Answer: ': 'pixie cut', 'BLEU Score: ': {'google_bleu': 1.0}, 'Bert Score: ': {'precision': [1.0], 'recall': [1.0], 'f1': [1.0], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 34\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What are the oven cleaning hacks that will have your appliance sparkling without ANY scrubbing?\n",
      "{'Reference Answer: ': 'homemade oven cleaner place the shelves in a resealable plastic bag, spray with oven cleaner, seal the bag, then leave to soak old toothbrush is an essential oven-cleaning tool glass scraper is ideal for removing tough stains remove greasy build-up on the hood of your oven with oil', 'Predicted Answer: ': '1. CREATE YOUR OWN CLEANER 2. USE WHITE OVEN', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.8424726724624634], 'recall': [0.7998151779174805], 'f1': [0.8205899000167847], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 35\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the \"this\" in the sentence referring to?\n",
      "{'Reference Answer: ': 'In February, when Rep. David Jolly introduced his quixotic plan to ban members of Congress from soliciting campaign contributions, the Florida Republican had only six co-sponsors.', 'Predicted Answer: ': 'eight', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.8177744746208191], 'recall': [0.793390154838562], 'f1': [0.8053978085517883], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 36\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What did he rescue?\n",
      "{'Reference Answer: ': 'southern flying squirrel', 'Predicted Answer: ': 'Jeff Longo was wandering in the intense Florida heat when he saw a tiny ball', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.8091810941696167], 'recall': [0.834778904914856], 'f1': [0.8217806816101074], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 37\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What are the 29 most beautiful TV quotes of all time?\n",
      "{'Reference Answer: ': '1. \"You can\\'t live your life according to maybes.\" 2. \"I wish there was a way to know you’re in the good old days before you’ve actually left them.\" 3. \"And what exactly do you think fairy tales are? They are a reminder that our lives will get better if we just hold on to hope. Your happy ending may not be what you expect, but that is what will make it so special.\" 4. \"To exist is to survive unfair choices.\" 5. \"I don’t want normal, and easy, and simple. I want painful, difficult, devastating, life-changing, extraordinary love. Don’t you want that, too?\"', 'Predicted Answer: ': '1. \"You can\\'t live your life according to maybes.\" 2. \"I wish there', 'BLEU Score: ': {'google_bleu': 0.13011152416356878}, 'Bert Score: ': {'precision': [0.9345920085906982], 'recall': [0.8314789533615112], 'f1': [0.880025327205658], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 38\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What happens just seconds later?\n",
      "{'Reference Answer: ': 'At this point, a large dog -- presumably Stella -- bounds into view, runs to one end of the fence and jumps right over it, almost without effort.', 'Predicted Answer: ': '\"Dammit.\"', 'BLEU Score: ': {'google_bleu': 0.00819672131147541}, 'Bert Score: ': {'precision': [0.8625673055648804], 'recall': [0.8076759576797485], 'f1': [0.8342196345329285], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 39\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the price of the thing that someone just paid $168000 for?\n",
      "{'Reference Answer: ': \"Hope's antique cabinet\", 'Predicted Answer: ': \"Hope's antique cabinet\", 'BLEU Score: ': {'google_bleu': 1.0}, 'Bert Score: ': {'precision': [0.9999998807907104], 'recall': [0.9999998807907104], 'f1': [0.9999998807907104], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 40\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What are the 14 things that'll happen in 2017 according to \"The Simpsons\"?\n",
      "{'Reference Answer: ': '1. Some alt-right guy will invent the make-up gun. 2. There will be a referendum on whether or not to deport illegal immigrants. 3. Greedy, corrupt energy firms will cause an environmental catastrophe, and a dome will be built over the contaminated site. 4. The chandelier in Elton John’s private jet will malfunction. 5. It will be made illegal to teach evolution in schools.', 'Predicted Answer: ': '1. Some alt-right guy will invent the make-up gun. 2. There will be', 'BLEU Score: ': {'google_bleu': 0.19463087248322147}, 'Bert Score: ': {'precision': [0.9518952369689941], 'recall': [0.8543654084205627], 'f1': [0.9004972577095032], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 41\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the main reason for the author's poor performance in sports during high school?\n",
      "{'Reference Answer: ': 'Some people are just better at sports than others', 'Predicted Answer: ': 'I was a horrible lacrosse player in high school: bad at catching the ball', 'BLEU Score: ': {'google_bleu': 0.018518518518518517}, 'Bert Score: ': {'precision': [0.87410569190979], 'recall': [0.845971405506134], 'f1': [0.8598084449768066], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 42\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the official departure date of Jon Stewart from The Daily Show?\n",
      "{'Reference Answer: ': 'August 6th', 'Predicted Answer: ': 'August 6', 'BLEU Score: ': {'google_bleu': 0.3333333333333333}, 'Bert Score: ': {'precision': [0.9240704774856567], 'recall': [0.8855890035629272], 'f1': [0.9044206142425537], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 43\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the size of Justin Bieber's penis?\n",
      "{'Reference Answer: ': 'perfectly average', 'Predicted Answer: ': 'huge dick', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.8981229066848755], 'recall': [0.8776376843452454], 'f1': [0.8877620697021484], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 44\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the word that should never follow \"I love you\"?\n",
      "{'Reference Answer: ': '\"but\"', 'Predicted Answer: ': 'I would love you, if only you would...\"', 'BLEU Score: ': {'google_bleu': 0.021739130434782608}, 'Bert Score: ': {'precision': [0.7967297434806824], 'recall': [0.8116495013237], 'f1': [0.8041204214096069], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 45\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the time for getting up close and personal with a rocket launch?\n",
      "{'Reference Answer: ': 'On Tuesday morning, NASA will broadcast its first-ever rocket launch livestream in 360-degree video', 'Predicted Answer: ': 'On Tuesday morning, NASA will broadcast its first-ever rocket launch livestream in 360-degree', 'BLEU Score: ': {'google_bleu': 0.9354838709677419}, 'Bert Score: ': {'precision': [0.9959357976913452], 'recall': [0.9891334772109985], 'f1': [0.9925230145454407], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 46\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the meaning of the sentence?\n",
      "{'Reference Answer: ': 'Vince Carter Paul Pierce Dirk Nowitzki', 'Predicted Answer: ': 'The twilight tales of Vince Carter, Dirk Nowitzki and Paul', 'BLEU Score: ': {'google_bleu': 0.18421052631578946}, 'Bert Score: ': {'precision': [0.881109893321991], 'recall': [0.9225620031356812], 'f1': [0.9013596177101135], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 47\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the problem that the sentence suggests the person solved?\n",
      "{'Reference Answer: ': 'Baking soda bee stings', 'Predicted Answer: ': '1. Mouthwash for your feet. 2. Black tea for sunburns. 3. Apple cider', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.8435438871383667], 'recall': [0.809343695640564], 'f1': [0.8260899186134338], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 48\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the newspaper?\n",
      "{'Reference Answer: ': 'The Arizona Republic', 'Predicted Answer: ': 'Arizona Republic', 'BLEU Score: ': {'google_bleu': 0.5}, 'Bert Score: ': {'precision': [0.935716986656189], 'recall': [0.8778082132339478], 'f1': [0.9058380722999573], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 49\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the meaning of the sentence?\n",
      "{'Reference Answer: ': 'I don’t think we should be starting to panic', 'Predicted Answer: ': 'Tasmanian facial tumor disease', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.7705353498458862], 'recall': [0.8254809379577637], 'f1': [0.7970623970031738], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 50\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the belief about the Syria conflict?\n",
      "{'Reference Answer: ': 'apocalyptic omen', 'Predicted Answer: ': 'I believe the battles in Syria are all part of the prophecies of the Book', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.8401151895523071], 'recall': [0.8130731582641602], 'f1': [0.8263730406761169], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 51\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What happened to the speaker's honeymoon?\n",
      "{'Reference Answer: ': 'Rudner and her husband rented ATVs. After hitting a speed bump and falling off of the vehicle, her husband shattered his shoulder and Rudner rotated her hip', 'Predicted Answer: ': 'At a stop in Mykonos during their cruise through the Greek Isles, Rud', 'BLEU Score: ': {'google_bleu': 0.02727272727272727}, 'Bert Score: ': {'precision': [0.819329023361206], 'recall': [0.8209238052368164], 'f1': [0.8201256394386292], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 52\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the drug that could add five years to a dog's life?\n",
      "{'Reference Answer: ': 'rapamycin', 'Predicted Answer: ': 'rapamycin', 'BLEU Score: ': {'google_bleu': 1.0}, 'Bert Score: ': {'precision': [1.0000001192092896], 'recall': [1.0000001192092896], 'f1': [1.0000001192092896], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 53\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the name of the Catholic university that has dropped abortion coverage?\n",
      "{'Reference Answer: ': 'Santa Clara University', 'Predicted Answer: ': 'Santa Clara University', 'BLEU Score: ': {'google_bleu': 1.0}, 'Bert Score: ': {'precision': [0.9999999403953552], 'recall': [0.9999999403953552], 'f1': [0.9999999403953552], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 54\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the president of the college trying to convey to the students by saying \"This is not a day care. This is a university!\"\n",
      "{'Reference Answer: ': 'Oklahoma Wesleyan University', 'Predicted Answer: ': 'Oklahoma Wesleyan University', 'BLEU Score: ': {'google_bleu': 1.0}, 'Bert Score: ': {'precision': [1.0], 'recall': [1.0], 'f1': [1.0], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 55\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the bedroom blues?\n",
      "{'Reference Answer: ': 'The sadness some men feel at this point may be due to the contrast between the joy of arousal and feeling like a superhero and the sensation of the feel-good hormones wearing off.', 'Predicted Answer: ': 'post-coital dysphoria', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.843337893486023], 'recall': [0.8333801031112671], 'f1': [0.8383294343948364], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 56\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the Amazon-Hachette war?\n",
      "{'Reference Answer: ': 'Edan Lepucki', 'Predicted Answer: ': 'Edan Lepucki’s Novel ‘California’ Gets', 'BLEU Score: ': {'google_bleu': 0.07142857142857142}, 'Bert Score: ': {'precision': [0.8618161678314209], 'recall': [0.9547595977783203], 'f1': [0.9059102535247803], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 57\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the occupation of the NFL player mentioned in the sentence?\n",
      "{'Reference Answer: ': 'Tim Masthay', 'Predicted Answer: ': 'Green Bay Packers punter Tim Masthay was obsessed with saving money before he even had', 'BLEU Score: ': {'google_bleu': 0.05555555555555555}, 'Bert Score: ': {'precision': [0.816917359828949], 'recall': [0.8980832099914551], 'f1': [0.8555795550346375], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 58\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the main point being made in this sentence?\n",
      "{'Reference Answer: ': 'But psychedelics may not be as dangerous and addictive as our society thinks.', 'Predicted Answer: ': 'Many of the negative perceptions we have of psychedelics can be traced back', 'BLEU Score: ': {'google_bleu': 0.04}, 'Bert Score: ': {'precision': [0.8779482841491699], 'recall': [0.8828686475753784], 'f1': [0.8804015517234802], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 59\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the weight of the orange object that can be seen in New York this week?\n",
      "{'Reference Answer: ': \"World's Largest Pumpkin\", 'Predicted Answer: ': '2,032 Pounds', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.7616909742355347], 'recall': [0.7873208522796631], 'f1': [0.7742938995361328], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 60\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What did Paris Hilton drop on two teeny-tiny dogs?\n",
      "{'Reference Answer: ': '$25K', 'Predicted Answer: ': 'Paris Hilton goes big when she goes small, dropping $25K on 2 dogs.', 'BLEU Score: ': {'google_bleu': 0.05172413793103448}, 'Bert Score: ': {'precision': [0.8179504871368408], 'recall': [0.8835495710372925], 'f1': [0.8494855165481567], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 61\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What did the guy do wrong?\n",
      "{'Reference Answer: ': 'If you find yourself so moved to store ice cream in your back pocket in Alabama, you’ll pay the price, and we don’t just mean a cold rear end.', 'Predicted Answer: ': '1. Feeding the homeless. 2. Never returning a video. 3. Giving a we', 'BLEU Score: ': {'google_bleu': 0.03278688524590164}, 'Bert Score: ': {'precision': [0.8527596592903137], 'recall': [0.8173755407333374], 'f1': [0.8346927165985107], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 62\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the method to completely eliminate stress in just three minutes?\n",
      "{'Reference Answer: ': 'close your eyes or simply stare at a fixed point Next, turn all focus to pulling in your breath Then, simply sigh Then start counting the breath in and out through the nose repeat this eight times', 'Predicted Answer: ': 'all you need to do is close your eyes or simply stare at a fixed point if', 'BLEU Score: ': {'google_bleu': 0.24}, 'Bert Score: ': {'precision': [0.888033390045166], 'recall': [0.845180869102478], 'f1': [0.8660773634910583], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 63\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the purpose of \"Sex Week\" at the university?\n",
      "{'Reference Answer: ': 'University of Tennessee', 'Predicted Answer: ': 'University of Tennessee', 'BLEU Score: ': {'google_bleu': 1.0}, 'Bert Score: ': {'precision': [1.0000001192092896], 'recall': [1.0000001192092896], 'f1': [1.0000001192092896], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 64\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What did the ex-basketball player do that made people angry?\n",
      "{'Reference Answer: ': 'Almario alleged physical abuse and uploading their sex video online', 'Predicted Answer: ': 'She broke up with him after he started abusing her', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.8687335252761841], 'recall': [0.8312049508094788], 'f1': [0.8495549559593201], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 65\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the most annoying thing on the entire planet?\n",
      "{'Reference Answer: ': 'Crazy Frog', 'Predicted Answer: ': 'Crazy Frog', 'BLEU Score: ': {'google_bleu': 1.0}, 'Bert Score: ': {'precision': [0.9999998807907104], 'recall': [0.9999998807907104], 'f1': [0.9999998807907104], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 66\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the main topic of the sentence?\n",
      "{'Reference Answer: ': '\"Although astrologers seek to explain the natural world, they don\\'t usually attempt to critically evaluate whether those explanations are valid — and this is a key part of science.\"', 'Predicted Answer: ': 'A hospital in Argentina is reportedly using astrology to help treat some mental health conditions,', 'BLEU Score: ': {'google_bleu': 0.023809523809523808}, 'Bert Score: ': {'precision': [0.8527121543884277], 'recall': [0.8436373472213745], 'f1': [0.8481504917144775], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 67\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the first thing guys notice about women?\n",
      "{'Reference Answer: ': 'face', 'Predicted Answer: ': 'your face', 'BLEU Score: ': {'google_bleu': 0.3333333333333333}, 'Bert Score: ': {'precision': [0.8476911187171936], 'recall': [0.9233378171920776], 'f1': [0.8838988542556763], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 68\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the name of the latest Hollywood baby?\n",
      "{'Reference Answer: ': 'Rachel Zoe Gives Birth To Her Second Baby Boy!', 'Predicted Answer: ': 'Rachel Zoe', 'BLEU Score: ': {'google_bleu': 0.08823529411764706}, 'Bert Score: ': {'precision': [0.8984192609786987], 'recall': [0.788816511631012], 'f1': [0.8400579690933228], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 69\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the name of the company that is crushing America's biggest clothing stores?\n",
      "{'Reference Answer: ': 'Amazon', 'Predicted Answer: ': 'Amazon', 'BLEU Score: ': {'google_bleu': 1.0}, 'Bert Score: ': {'precision': [1.0], 'recall': [1.0], 'f1': [1.0], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 70\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What are the shower habits that you need to stop doing immediately?\n",
      "{'Reference Answer: ': '1. Washing Your Face 2. Not Washing Your Feet 3. Not Washing or Replacing Your Loofah Regularly 4. Using a Soap Dish 5. Using Scented Soaps', 'Predicted Answer: ': '1. Washing Your Face 2. Snacking 3. Letting Go 4. Snacking', 'BLEU Score: ': {'google_bleu': 0.23728813559322035}, 'Bert Score: ': {'precision': [0.9134860038757324], 'recall': [0.8686689734458923], 'f1': [0.8905138969421387], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 71\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the disease mentioned in the sentence?\n",
      "{'Reference Answer: ': 'kuru', 'Predicted Answer: ': 'Sorcery', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.8566282987594604], 'recall': [0.805628776550293], 'f1': [0.8303461670875549], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 72\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the name of the dangerous plant that can kill children?\n",
      "{'Reference Answer: ': 'Dieffenbachia', 'Predicted Answer: ': 'Dieffenbachia', 'BLEU Score: ': {'google_bleu': 1.0}, 'Bert Score: ': {'precision': [1.0], 'recall': [1.0], 'f1': [1.0], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 73\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " Are left-handed people really more creative than right-handed people?\n",
      "{'Reference Answer: ': 'The answer to that is a definitive ... maybe.', 'Predicted Answer: ': 'Scientists have been chipping away at the peculiarities of left-handedness, which occurs', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.8300566077232361], 'recall': [0.7924078702926636], 'f1': [0.8107954859733582], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 74\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the name of the \"unknown substance\" that sent 7 Planned Parenthood staffers to the hospital?\n",
      "{'Reference Answer: ': 'baby food', 'Predicted Answer: ': 'baby food', 'BLEU Score: ': {'google_bleu': 1.0}, 'Bert Score: ': {'precision': [0.9999998807907104], 'recall': [0.9999998807907104], 'f1': [0.9999998807907104], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 75\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What has Michael Douglass' son Cameron tattooed on his abs?\n",
      "{'Reference Answer: ': 'Michael and Kirk', 'Predicted Answer: ': 'Michael and Kirk', 'BLEU Score: ': {'google_bleu': 1.0}, 'Bert Score: ': {'precision': [1.0], 'recall': [1.0], 'f1': [1.0], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 76\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the cancer-causing ingredient lurking in your beauty products?\n",
      "{'Reference Answer: ': '1,4-dioxane', 'Predicted Answer: ': '1,4-dioxane', 'BLEU Score: ': {'google_bleu': 1.0}, 'Bert Score: ': {'precision': [1.0], 'recall': [1.0], 'f1': [1.0], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 77\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the surprising thing that may be making you crave junk food?\n",
      "{'Reference Answer: ': 'gut bacteria', 'Predicted Answer: ': 'yogurt', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.9043344855308533], 'recall': [0.8356538414955139], 'f1': [0.8686386942863464], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 78\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What does it feel like to die?\n",
      "{'Reference Answer: ': 'from the last two weeks until the last breath, somewhere in that interval, people become too sick, or too drowsy, or too unconscious, to tell us what they’re experiencing Pre-death dreams were frequently so intense that the dream carried into wakefulness. First hunger and then thirst are lost. Speech is lost next, followed by vision. The last senses to go are usually hearing and touch. There are some kinds of conditions where pain is inevitable We generally believe that if your brain is really in a comatose kind of situation, or you’re not really responsive, that your perception—how you feel about things—may also be significantly decreased,', 'Predicted Answer: ': '\"I don’t know if there will be any physical changes, but I know that', 'BLEU Score: ': {'google_bleu': 0.00851063829787234}, 'Bert Score: ': {'precision': [0.8312759399414062], 'recall': [0.7959201335906982], 'f1': [0.8132139444351196], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 79\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What state is NOT excited about legal weed?\n",
      "{'Reference Answer: ': 'Wyoming', 'Predicted Answer: ': 'Wyoming', 'BLEU Score: ': {'google_bleu': 1.0}, 'Bert Score: ': {'precision': [1.0000001192092896], 'recall': [1.0000001192092896], 'f1': [1.0000001192092896], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 80\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the company's policy on bringing pets to work?\n",
      "{'Reference Answer: ': 'Nvidia', 'Predicted Answer: ': 'Nvidia allows and even encourages its workers to bring their dogs to work', 'BLEU Score: ': {'google_bleu': 0.021739130434782608}, 'Bert Score: ': {'precision': [0.792341947555542], 'recall': [0.8389900326728821], 'f1': [0.8149990439414978], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 81\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the reason behind the sudden absence of certain letters in big brand names?\n",
      "{'Reference Answer: ': \"it's all part of a massive effort to encourage people around the globe to donate blood\", 'Predicted Answer: ': \"A's, B's and O's\", 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.782738208770752], 'recall': [0.8020256757736206], 'f1': [0.7922645807266235], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 82\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the answer to the question posed in the sentence?\n",
      "{'Reference Answer: ': 'Republican voters would enthusiastically welcome a black candidate, a Donell Trump — so long as he, too, championed nationalist, politically incorrect, anti-immigrant populism.', 'Predicted Answer: ': 'If Donald Trump were black, would the GOP base accept him?', 'BLEU Score: ': {'google_bleu': 0.03636363636363636}, 'Bert Score: ': {'precision': [0.8925756812095642], 'recall': [0.8593295812606812], 'f1': [0.8756372332572937], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 83\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What do Americans drink every day?\n",
      "{'Reference Answer: ': '2.1 coffee drinks', 'Predicted Answer: ': '2.1 coffee drinks per day', 'BLEU Score: ': {'google_bleu': 0.42857142857142855}, 'Bert Score: ': {'precision': [0.9058703184127808], 'recall': [0.9294469356536865], 'f1': [0.9175072312355042], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 84\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the reason behind this model suing for $1.5 billion?\n",
      "{'Reference Answer: ': 'Yuliana Avalos Match.com’s parent company, IAC (InterActiveCorp)', 'Predicted Answer: ': 'Yuliana Avalos', 'BLEU Score: ': {'google_bleu': 0.07142857142857142}, 'Bert Score: ': {'precision': [0.9428043961524963], 'recall': [0.8386964797973633], 'f1': [0.8877084851264954], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 85\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the claim made in the sentence?\n",
      "{'Reference Answer: ': 'you’d still have a hard time arguing they were even the worst band on this stage', 'Predicted Answer: ': \"Red Hot Chili Peppers' drummer is a 54 year-old man who’s never\", 'BLEU Score: ': {'google_bleu': 0.017241379310344827}, 'Bert Score: ': {'precision': [0.8080025911331177], 'recall': [0.8361846208572388], 'f1': [0.8218520879745483], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 86\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What was the strategy that Obama's female staffers came up with to ensure their voices were heard?\n",
      "{'Reference Answer: ': 'Female staffers adopted a meeting strategy they called \"amplification\": When a woman made a key point, other women would repeat it, giving credit to its author.', 'Predicted Answer: ': 'the White House', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.8058403730392456], 'recall': [0.7988884449005127], 'f1': [0.802349328994751], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 87\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What are the ways in which Beats Electronics tricks consumers into thinking that its products are premium?\n",
      "{'Reference Answer: ': 'In these headphones, 30% of the weight comes from four tiny metal parts that are there for the sole purpose of adding weight', 'Predicted Answer: ': 'cheaply made The headphones are incredibly cheaply made The company cuts corners everywhere it can The', 'BLEU Score: ': {'google_bleu': 0.02127659574468085}, 'Bert Score: ': {'precision': [0.847265899181366], 'recall': [0.8487456440925598], 'f1': [0.8480051159858704], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 88\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the clever trick that will instantly clear your lawn of unwanted leaves?\n",
      "{'Reference Answer: ': 'giant piece of cardboard', 'Predicted Answer: ': 'cardboard', 'BLEU Score: ': {'google_bleu': 0.1}, 'Bert Score: ': {'precision': [0.8743352293968201], 'recall': [0.7852595448493958], 'f1': [0.8274068832397461], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 89\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the theory about why Donald Trump keeps winning?\n",
      "{'Reference Answer: ': 'Solomon’s recent research shows that people who are thinking about death are more likely to say they support him', 'Predicted Answer: ': 'people who are thinking about death are more likely to say they support him', 'BLEU Score: ': {'google_bleu': 0.7142857142857143}, 'Bert Score: ': {'precision': [0.9682577848434448], 'recall': [0.9354591369628906], 'f1': [0.9515759348869324], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 90\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What are some of the curious cases and unsolved mysteries that perplexed us in 2016?\n",
      "{'Reference Answer: ': 'Mark and Jacoba Tromp and their adult children Ella, Riana and Mitchell left their home in Silvan, east of Melbourne, taking cash but leaving behind bank cards and mobile phones. Hundreds of kangaroos were found dead in far west New South Wales this year from what was described as a \"mystery disease\". When a magnitude 7.8 earthquake struck near Christchurch in November, videos emerged that appeared to show the New Zealand sky lighting up in blue and green. This year it was reported that doctors were at a loss to explain the mysterious illness making a four-year-old Bangladeshi boy look like an old man. The FBI announced this year that one of America\\'s most baffling crimes — that of hijacker Dan \"DB Cooper\" who jumped out of a plane with a parachute and ransom money 45 years ago — looks set to remain an enigma.', 'Predicted Answer: ': 'Mark and Jacoba Tromp and their adult children Ella, Riana and Mitchell left their', 'BLEU Score: ': {'google_bleu': 0.08626198083067092}, 'Bert Score: ': {'precision': [0.9582487344741821], 'recall': [0.7836066484451294], 'f1': [0.8621727228164673], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 91\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What are the worst jobs that ex-cons remember after being released from prison?\n",
      "{'Reference Answer: ': 'working on a cattle ranch as a utility farmer, picking up trash at the city dump in 100-plus degree weather telemarketing place', 'Predicted Answer: ': 'getting a job', 'BLEU Score: ': {'google_bleu': 0.010638297872340425}, 'Bert Score: ': {'precision': [0.8618618845939636], 'recall': [0.7996289134025574], 'f1': [0.8295799493789673], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 92\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the issue with the internet ruling that Verizon and AT&T don't like?\n",
      "{'Reference Answer: ': 'net neutrality', 'Predicted Answer: ': 'A seemingly obscure legal decision was reached this week, but it affects our use and enjoyment of', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.8046607375144958], 'recall': [0.7951395511627197], 'f1': [0.7998718023300171], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 93\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What did this teen do instead of giving a graduation speech?\n",
      "{'Reference Answer: ': 'sang his very own special rendition of Lukas Graham’s hit song \"7 Years.\"', 'Predicted Answer: ': 'JP Wallace sang his very own special rendition of Lukas Graham’s hit song \"', 'BLEU Score: ': {'google_bleu': 0.7241379310344828}, 'Bert Score: ': {'precision': [0.9374668002128601], 'recall': [0.9498843550682068], 'f1': [0.9436347484588623], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 94\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the title of the next Star Wars movie?\n",
      "{'Reference Answer: ': 'Fall of the Resistance.', 'Predicted Answer: ': 'Fall of the Resistance', 'BLEU Score: ': {'google_bleu': 0.7142857142857143}, 'Bert Score: ': {'precision': [0.980004072189331], 'recall': [0.983994722366333], 'f1': [0.9819953441619873], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 95\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What did we figure out when the moon formed?\n",
      "{'Reference Answer: ': '4.47 billion years ago', 'Predicted Answer: ': 'collision of two protoplanets, or embryonic worlds', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.8075591325759888], 'recall': [0.8058575391769409], 'f1': [0.8067074418067932], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 96\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What country recently decriminalized marijuana?\n",
      "{'Reference Answer: ': 'Switzerland', 'Predicted Answer: ': 'Switzerland', 'BLEU Score: ': {'google_bleu': 1.0}, 'Bert Score: ': {'precision': [0.9999999403953552], 'recall': [0.9999999403953552], 'f1': [0.9999999403953552], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 97\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the disturbing reason?\n",
      "{'Reference Answer: ': 'They are soaked in a bath of chlorine', 'Predicted Answer: ': 'They could make you very ill', 'BLEU Score: ': {'google_bleu': 0.038461538461538464}, 'Bert Score: ': {'precision': [0.8475748300552368], 'recall': [0.8505369424819946], 'f1': [0.8490532636642456], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 98\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the iPhone 5C number that Apple doesn't want you to know?\n",
      "{'Reference Answer: ': 'preorder figure was 2 million', 'Predicted Answer: ': 'number', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.8293511271476746], 'recall': [0.7952939867973328], 'f1': [0.8119655847549438], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      "-------- 99\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: \n",
      " What is the reason the mother glued pennies to her daughter's shoes?\n",
      "{'Reference Answer: ': 'make-shift tap shoes', 'Predicted Answer: ': 'Knowing her daughter’s love for dancing and playing, she decided to take some of her old', 'BLEU Score: ': {'google_bleu': 0.0}, 'Bert Score: ': {'precision': [0.8247225284576416], 'recall': [0.8150407671928406], 'f1': [0.8198530673980713], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.41.0.dev0)'}}\n",
      " All 0.8919407784938812\n",
      " Phrase 0.4297121506929398\n",
      " Passage 0.290065113902092\n",
      " Multi 0.17216351389884949\n"
     ]
    }
   ],
   "source": [
    "_all = {}\n",
    "arr = [{} for i in range(3)]\n",
    "\n",
    "for i in range(100):\n",
    "    print('--------', i)\n",
    "    question = validation_dataset.iloc[i]['generated_que']\n",
    "    context = validation_dataset.iloc[i]['article']\n",
    "    answer = ' '.join(validation_dataset.iloc[i]['spoiler'])\n",
    "        \n",
    "    print(validation_dataset.iloc[i]['labels'])\n",
    "    ans = predict_answer(context, question, answer)\n",
    "    _all['bleu'] = _all.get('bleu', 0) + ans[\"BLEU Score: \"]['google_bleu']\n",
    "    _all['bert'] = _all.get('bert', 0) + ans['Bert Score: ']['precision'][0]\n",
    "\n",
    "\n",
    "    type = validation_dataset.iloc[i]['labels']\n",
    "    arr[type]['bleu'] = arr[type].get('bleu',0) + ans[\"BLEU Score: \"]['google_bleu']\n",
    "    arr[type]['bert'] = arr[type].get('bert',0) + ans[\"Bert Score: \"]['precision'][0]\n",
    "        \n",
    "        \n",
    "    \n",
    "    print(ans)\n",
    "\n",
    "print(\" All \" + str(_all['bert']/100))\n",
    "print(\" Phrase \" + str(arr[0]['bert']/100))\n",
    "print(\" Passage \" + str(arr[1]['bert']/100))\n",
    "print(\" Multi \" + str(arr[2]['bert']/100))\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jupyter] *",
   "language": "python",
   "name": "conda-env-jupyter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
